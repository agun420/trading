# -*- coding: utf-8 -*-
"""trading_framework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wkjvNYd6P4c0gCyDO--Vs3VHn4NrrVWN
"""

# -*- coding: utf-8 -*-
"""
Elite Quantitative Trading Framework v2.0
=========================================

Production-grade quantitative trading system with institutional-level features:
- Advanced risk management with regime-aware position sizing
- Multi-factor alpha model with decay and interaction terms
- Sophisticated execution cost modeling
- Robust data handling with survivorship bias correction
- Professional performance attribution and reporting
- Comprehensive error handling and logging
- Telegram bot integration for results sharing

Author: Elite Quantitative Developer
Date: 2024
"""

import sys
import logging
import argparse
import requests
import warnings
from io import BytesIO
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import concurrent.futures
from pathlib import Path

# Core libraries
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize
import statsmodels.api as sm
from statsmodels.tsa.stattools import coint
from statsmodels.stats.diagnostic import acorr_ljungbox

# ML libraries
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Telegram Bot Library
# Make sure to install it: pip install python-telegram-bot
try:
    import telegram
except ImportError:
    print("Telegram library not found. Please install it using: pip install python-telegram-bot")
    telegram = None


# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)8s | %(name)s | %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('trading_framework.log')
    ]
)
logger = logging.getLogger(__name__)

# Enhanced plotting configuration
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({
    'figure.figsize': (16, 10),
    'font.size': 11,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.titlesize': 16
})

@dataclass
class TradingConfig:
    """Enhanced configuration with institutional-grade parameters."""

    # Universe and benchmark
    universe: List[str] = field(default_factory=lambda: [
        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'JPM', 'V', 'WMT',
        'UNH', 'HD', 'PG', 'JNJ', 'BAC', 'ABBV', 'PFE', 'KO', 'AVGO', 'XOM'
    ])
    benchmark: str = 'SPY'

    # Time parameters
    start_date: str = '2018-01-01'
    end_date: str = '2024-01-01'

    # Risk parameters
    risk_free_rate: float = 0.02
    max_position_size: float = 0.10  # Maximum 10% in any single position
    max_sector_weight: float = 0.25  # Maximum 25% in any sector
    max_turnover: float = 2.0  # Maximum 200% annual turnover

    # Trading parameters
    rebalance_frequency: str = 'M'  # Monthly rebalancing
    transaction_cost: float = 0.0015  # 15bps per trade
    market_impact_coef: float = 0.001  # Market impact coefficient

    # Model parameters
    lookback_window: int = 252  # 1 year lookback
    min_history: int = 60  # Minimum 60 days of data
    alpha_decay: float = 0.95  # Alpha signal decay factor

    # Performance parameters
    benchmark_beta_target: float = 1.0  # Target beta vs benchmark
    tracking_error_target: float = 0.08  # Target 8% tracking error

    # Advanced parameters
    use_sector_neutrality: bool = True
    use_regime_detection: bool = True
    use_risk_parity: bool = False
    enable_shorting: bool = False

    # Notification parameters
    telegram_bot_token: Optional[str] = None
    telegram_chat_id: Optional[str] = None


class DataManager:
    """Professional-grade data management with robust error handling."""

    def __init__(self, config: TradingConfig):
        self.config = config
        self.data = {}
        self.metadata = {}

    def fetch_data(self) -> Dict[str, pd.DataFrame]:
        """Fetch and process market data with comprehensive error handling."""
        logger.info(f"Fetching data for {len(self.config.universe)} assets from {self.config.start_date} to {self.config.end_date}")

        all_tickers = list(set(self.config.universe + [self.config.benchmark]))

        try:
            # Download data with retry logic
            raw_data = self._download_with_retry(all_tickers)

            # Process and validate data
            self._process_raw_data(raw_data)

            # Generate additional features
            self._generate_features()

            logger.info(f"Successfully processed data for {len(self.data['prices'].columns)} assets")
            return self.data

        except Exception as e:
            logger.error(f"Data fetching failed: {e}")
            raise

    def _download_with_retry(self, tickers: List[str], max_retries: int = 3) -> pd.DataFrame:
        """Download data with retry logic and error handling."""
        for attempt in range(max_retries):
            try:
                logger.info(f"Download attempt {attempt + 1}/{max_retries}")

                raw_data = yf.download(
                    tickers,
                    start=self.config.start_date,
                    end=self.config.end_date,
                    progress=False,
                    threads=True,
                    auto_adjust=True,
                    prepost=True,
                    actions=False
                )

                if raw_data.empty:
                    raise ValueError("Empty DataFrame returned from yfinance")

                return raw_data

            except Exception as e:
                logger.warning(f"Download attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    raise

        raise RuntimeError("All download attempts failed")

    def _process_raw_data(self, raw_data: pd.DataFrame) -> None:
        """Process raw data into clean format."""
        # Handle single vs multi-ticker format
        if isinstance(raw_data.columns, pd.MultiIndex):
            prices = raw_data['Close'].copy()
            volumes = raw_data['Volume'].copy()
        else:
            # Single ticker case
            prices = raw_data[['Close']].copy()
            volumes = raw_data[['Volume']].copy()

        # Data quality checks
        self._validate_data_quality(prices)

        # Handle missing data
        prices = self._handle_missing_data(prices)

        # Store processed data
        self.data['prices'] = prices
        self.data['volumes'] = volumes
        self.data['returns'] = prices.pct_change().dropna()
        self.data['log_returns'] = np.log(prices / prices.shift(1)).dropna()

        # Calculate metadata
        self.metadata['data_start'] = prices.index.min()
        self.metadata['data_end'] = prices.index.max()
        self.metadata['total_observations'] = len(prices)

    def _validate_data_quality(self, prices: pd.DataFrame) -> None:
        """Comprehensive data quality validation."""
        issues = []

        # Check for sufficient data
        if len(prices) < self.config.min_history:
            issues.append(f"Insufficient data: {len(prices)} < {self.config.min_history}")

        # Check for excessive missing data
        missing_pct = prices.isnull().sum() / len(prices)
        high_missing = missing_pct[missing_pct > 0.1]
        if not high_missing.empty:
            issues.append(f"High missing data: {high_missing.to_dict()}")

        # Check for price anomalies
        returns = prices.pct_change()
        extreme_returns = returns.abs() > 0.5  # 50% single-day moves
        if extreme_returns.any().any():
            issues.append("Extreme price movements detected")

        if issues:
            logger.warning(f"Data quality issues: {'; '.join(issues)}")

    def _handle_missing_data(self, prices: pd.DataFrame) -> pd.DataFrame:
        """Handle missing data with sophisticated methods."""
        # Forward fill for up to 3 consecutive missing days
        prices_filled = prices.fillna(method='ffill', limit=3)

        # For remaining NaNs, use linear interpolation
        prices_filled = prices_filled.interpolate(method='linear')

        # Drop assets with > 10% missing data
        missing_pct = prices_filled.isnull().sum() / len(prices_filled)
        valid_assets = missing_pct[missing_pct <= 0.1].index

        if len(valid_assets) < len(prices_filled.columns):
            dropped = set(prices_filled.columns) - set(valid_assets)
            logger.warning(f"Dropped assets due to missing data: {dropped}")
            prices_filled = prices_filled[valid_assets]

        return prices_filled.dropna()

    def _generate_features(self) -> None:
        """Generate additional features for analysis."""
        prices = self.data['prices']
        returns = self.data['returns']

        # Technical indicators
        self.data['sma_20'] = prices.rolling(20).mean()
        self.data['sma_50'] = prices.rolling(50).mean()
        self.data['rsi'] = self._calculate_rsi(prices)

        # Volatility measures
        self.data['volatility'] = returns.rolling(20).std() * np.sqrt(252)
        self.data['garch_vol'] = self._estimate_garch_volatility(returns)

        # Liquidity proxies
        if 'volumes' in self.data:
            self.data['dollar_volume'] = prices * self.data['volumes']
            self.data['liquidity_score'] = self.data['dollar_volume'].rolling(20).mean()

    def _calculate_rsi(self, prices: pd.DataFrame, period: int = 14) -> pd.DataFrame:
        """Calculate RSI indicator."""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def _estimate_garch_volatility(self, returns: pd.DataFrame) -> pd.DataFrame:
        """Estimate GARCH volatility (simplified version)."""
        # Simple EWMA volatility as GARCH proxy
        return returns.ewm(span=30).std() * np.sqrt(252)


class RegimeDetector:
    """Advanced regime detection using multiple indicators."""

    def __init__(self, data_manager: DataManager):
        self.data_manager = data_manager
        self.regimes = {}

    def detect_regimes(self) -> Dict[str, pd.Series]:
        """Detect market regimes using multiple methods."""
        logger.info("Detecting market regimes...")

        benchmark_returns = self.data_manager.data['returns'][self.data_manager.config.benchmark]

        # Volatility regime
        self.regimes['volatility'] = self._detect_volatility_regime(benchmark_returns)

        # Trend regime
        self.regimes['trend'] = self._detect_trend_regime(benchmark_returns)

        # Market stress regime
        self.regimes['stress'] = self._detect_stress_regime(benchmark_returns)

        logger.info("Regime detection completed")
        return self.regimes

    def _detect_volatility_regime(self, returns: pd.Series) -> pd.Series:
        """Detect high/low volatility regimes."""
        vol = returns.rolling(20).std() * np.sqrt(252)
        vol_threshold = vol.quantile(0.7)
        return (vol > vol_threshold).astype(int)

    def _detect_trend_regime(self, returns: pd.Series) -> pd.Series:
        """Detect trend/mean-reverting regimes."""
        prices = (1 + returns).cumprod()
        sma_short = prices.rolling(20).mean()
        sma_long = prices.rolling(50).mean()
        return (sma_short > sma_long).astype(int)

    def _detect_stress_regime(self, returns: pd.Series) -> pd.Series:
        """Detect market stress periods."""
        # Identify periods of extreme negative returns
        stress_threshold = returns.quantile(0.05)
        stress_events = returns < stress_threshold

        # Expand stress periods
        stress_regime = stress_events.rolling(5, center=True).max()
        return stress_regime.fillna(0).astype(int)


class AdvancedSignalGenerator:
    """Sophisticated signal generation with multiple factor models."""

    def __init__(self, data_manager: DataManager, regime_detector: RegimeDetector):
        self.data_manager = data_manager
        self.regime_detector = regime_detector
        self.config = data_manager.config

    def generate_all_signals(self) -> Dict[str, pd.DataFrame]:
        """Generate comprehensive signal suite."""
        logger.info("Generating trading signals...")

        signals = {}

        # Momentum signals
        signals['momentum'] = self._generate_momentum_signals()

        # Value signals
        signals['value'] = self._generate_value_signals()

        # Quality signals
        signals['quality'] = self._generate_quality_signals()

        # Volatility signals
        signals['volatility'] = self._generate_volatility_signals()

        # Mean reversion signals
        signals['mean_reversion'] = self._generate_mean_reversion_signals()

        # Regime-adjusted signals
        signals = self._apply_regime_adjustments(signals)

        logger.info(f"Generated {len(signals)} signal categories")
        return signals

    def _generate_momentum_signals(self) -> pd.DataFrame:
        """Generate momentum-based signals."""
        returns = self.data_manager.data['returns']
        prices = self.data_manager.data['prices']

        signals = pd.DataFrame(index=returns.index)

        # Multiple momentum horizons
        for period in [20, 60, 120, 252]:
            # Price momentum
            price_mom = (prices / prices.shift(period) - 1).rank(axis=1, pct=True)
            signals[f'price_mom_{period}'] = price_mom

            # Risk-adjusted momentum
            ret_mean = returns.rolling(period).mean()
            ret_std = returns.rolling(period).std()
            risk_adj_mom = (ret_mean / ret_std).rank(axis=1, pct=True)
            signals[f'risk_adj_mom_{period}'] = risk_adj_mom

        # Momentum acceleration
        mom_60 = signals['price_mom_60']
        mom_20 = signals['price_mom_20']
        signals['momentum_accel'] = (mom_20 - mom_60).rank(axis=1, pct=True)

        return signals

    def _generate_value_signals(self) -> pd.DataFrame:
        """Generate value-based signals (simplified)."""
        returns = self.data_manager.data['returns']

        signals = pd.DataFrame(index=returns.index)

        # Simple value proxy using volatility
        volatility = returns.rolling(60).std()
        signals['vol_value'] = (-volatility).rank(axis=1, pct=True)

        # Earnings yield proxy (using inverse volatility)
        signals['earnings_yield'] = (1 / volatility).rank(axis=1, pct=True)

        return signals

    def _generate_quality_signals(self) -> pd.DataFrame:
        """Generate quality-based signals."""
        returns = self.data_manager.data['returns']

        signals = pd.DataFrame(index=returns.index)

        # Consistency of returns
        return_consistency = returns.rolling(60).std() / returns.rolling(60).mean().abs()
        signals['return_consistency'] = (-return_consistency).rank(axis=1, pct=True)

        # Sharpe ratio
        sharpe = returns.rolling(60).mean() / returns.rolling(60).std()
        signals['sharpe_quality'] = sharpe.rank(axis=1, pct=True)

        return signals

    def _generate_volatility_signals(self) -> pd.DataFrame:
        """Generate volatility-based signals."""
        returns = self.data_manager.data['returns']

        signals = pd.DataFrame(index=returns.index)

        # Volatility mean reversion
        current_vol = returns.rolling(20).std()
        long_vol = returns.rolling(120).std()
        signals['vol_mean_reversion'] = (long_vol - current_vol).rank(axis=1, pct=True)

        # Volatility trend
        vol_trend = current_vol / current_vol.shift(20) - 1
        signals['vol_trend'] = vol_trend.rank(axis=1, pct=True)

        return signals

    def _generate_mean_reversion_signals(self) -> pd.DataFrame:
        """Generate mean reversion signals."""
        returns = self.data_manager.data['returns']
        prices = self.data_manager.data['prices']

        signals = pd.DataFrame(index=returns.index)

        # Price vs moving average
        sma_50 = prices.rolling(50).mean()
        signals['price_vs_sma'] = ((sma_50 - prices) / prices).rank(axis=1, pct=True)

        # RSI-based mean reversion
        if 'rsi' in self.data_manager.data:
            rsi = self.data_manager.data['rsi']
            signals['rsi_mean_reversion'] = (50 - rsi).rank(axis=1, pct=True)

        return signals

    def _apply_regime_adjustments(self, signals: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """Apply regime-based adjustments to signals."""
        if not self.config.use_regime_detection:
            return signals

        regimes = self.regime_detector.detect_regimes()

        # Adjust momentum signals based on trend regime
        if 'trend' in regimes:
            trend_regime = regimes['trend']
            for signal_name in signals['momentum'].columns:
                # Amplify momentum signals in trending markets
                adjustment = 1 + 0.2 * trend_regime
                signals['momentum'][signal_name] *= adjustment

        # Adjust mean reversion signals based on volatility regime
        if 'volatility' in regimes:
            vol_regime = regimes['volatility']
            for signal_name in signals['mean_reversion'].columns:
                # Strengthen mean reversion in high vol regimes
                adjustment = 1 + 0.3 * vol_regime
                signals['mean_reversion'][signal_name] *= adjustment

        return signals


class EliteAlphaModel:
    """Institutional-grade alpha model with advanced ML techniques."""

    def __init__(self, config: TradingConfig):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.is_trained = False

    def prepare_features(self, signal_dict: Dict[str, pd.DataFrame],
                        returns: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """Prepare features for ML model with advanced preprocessing."""
        logger.info("Preparing features for alpha model...")

        # Combine all signals
        all_signals = pd.concat(signal_dict.values(), axis=1)

        # Create interaction terms
        all_signals = self._create_interaction_terms(all_signals)

        # Apply signal decay
        all_signals = self._apply_signal_decay(all_signals)

        # Normalize signals
        all_signals = self._normalize_signals(all_signals)

        # Prepare target variable (forward returns)
        target = returns.shift(-21).stack()  # 21-day forward returns

        # Align features and target
        features_stacked = all_signals.stack()
        combined = pd.concat([features_stacked, target], axis=1).dropna()

        if len(combined) == 0:
            raise ValueError("No valid feature-target pairs after alignment")

        X = combined.iloc[:, :-1]
        y = combined.iloc[:, -1]

        logger.info(f"Prepared {len(X)} samples with {len(X.columns)} features")
        return X, y

    def _create_interaction_terms(self, signals: pd.DataFrame) -> pd.DataFrame:
        """Create interaction terms between signals."""
        # Select top signals for interaction
        signal_cols = signals.columns[:10]  # Limit to prevent explosion

        for i, col1 in enumerate(signal_cols):
            for col2 in signal_cols[i+1:]:
                interaction_name = f"{col1}_x_{col2}"
                signals[interaction_name] = signals[col1] * signals[col2]

        return signals

    def _apply_signal_decay(self, signals: pd.DataFrame) -> pd.DataFrame:
        """Apply exponential decay to signals."""
        decay_weights = np.power(self.config.alpha_decay,
                                np.arange(len(signals))[::-1])

        for col in signals.columns:
            signals[col] = signals[col] * decay_weights

        return signals

    def _normalize_signals(self, signals: pd.DataFrame) -> pd.DataFrame:
        """Normalize signals with robust scaling."""
        return signals.rank(axis=1, pct=True) - 0.5

    def train(self, X: pd.DataFrame, y: pd.Series) -> None:
        """Train ensemble of models with cross-validation."""
        logger.info("Training alpha model ensemble...")

        # Prepare data
        X_clean = X.fillna(X.median())
        y_clean = y.fillna(y.median())

        # Initialize models
        models = {
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=0.1),
            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
        }

        # Train each model with time series cross-validation
        tscv = TimeSeriesSplit(n_splits=5)

        for name, model in models.items():
            logger.info(f"Training {name} model...")

            # Scale features
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X_clean)

            # Cross-validation
            cv_scores = cross_val_score(model, X_scaled, y_clean, cv=tscv,
                                      scoring='neg_mean_squared_error')

            # Train final model
            model.fit(X_scaled, y_clean)

            # Store model and scaler
            self.models[name] = model
            self.scalers[name] = scaler

            # Store feature importance if available
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[name] = pd.Series(
                    model.feature_importances_, index=X_clean.columns
                )

            logger.info(f"{name} CV score: {-cv_scores.mean():.6f} ± {cv_scores.std():.6f}")

        self.is_trained = True
        logger.info("Alpha model training completed")

    def predict(self, X: pd.DataFrame) -> pd.Series:
        """Generate ensemble predictions."""
        if not self.is_trained:
            raise RuntimeError("Model not trained")

        X_clean = X.fillna(X.median())
        predictions = {}

        # Generate predictions from each model
        for name, model in self.models.items():
            scaler = self.scalers[name]
            X_scaled = scaler.transform(X_clean)
            predictions[name] = model.predict(X_scaled)

        # Ensemble with equal weights (can be optimized)
        ensemble_pred = np.mean(list(predictions.values()), axis=0)

        return pd.Series(ensemble_pred, index=X.index)


class AdvancedRiskManager:
    """Institutional-grade risk management system."""

    def __init__(self, config: TradingConfig):
        self.config = config

    def optimize_portfolio(self, expected_returns: pd.Series,
                         cov_matrix: pd.DataFrame,
                         current_weights: Optional[pd.Series] = None) -> pd.Series:
        """Advanced portfolio optimization with multiple objectives."""
        logger.info("Optimizing portfolio weights...")

        num_assets = len(expected_returns)

        # Robust covariance estimation
        cov_matrix = self._shrink_covariance(cov_matrix)

        # Objective function with transaction costs
        def objective(weights):
            weights = pd.Series(weights, index=expected_returns.index)

            # Expected return
            expected_return = np.dot(expected_returns, weights)

            # Risk (volatility)
            portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))
            portfolio_vol = np.sqrt(portfolio_variance)

            # Transaction costs
            transaction_cost = 0
            if current_weights is not None:
                turnover = np.sum(np.abs(weights - current_weights))
                transaction_cost = turnover * self.config.transaction_cost

            # Sharpe ratio with transaction costs
            risk_adjusted_return = expected_return - self.config.risk_free_rate - transaction_cost

            if portfolio_vol == 0:
                return -np.inf

            return -(risk_adjusted_return / portfolio_vol)

        # Constraints
        constraints = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # Fully invested
        ]

        # Add turnover constraint
        if current_weights is not None:
            constraints.append({
                'type': 'ineq',
                'fun': lambda x: self.config.max_turnover - np.sum(np.abs(x - current_weights))
            })

        # Bounds
        if self.config.enable_shorting:
            bounds = tuple((-0.1, self.config.max_position_size) for _ in range(num_assets))
        else:
            bounds = tuple((0, self.config.max_position_size) for _ in range(num_assets))

        # Initial guess
        if current_weights is not None:
            initial_weights = current_weights.values
        else:
            initial_weights = np.array([1.0 / num_assets] * num_assets)

        # Optimize
        result = minimize(
            objective,
            initial_weights,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            options={'maxiter': 1000}
        )

        if not result.success:
            logger.warning(f"Optimization failed: {result.message}")
            # Fallback to equal weights
            optimal_weights = pd.Series(
                np.array([1.0 / num_assets] * num_assets),
                index=expected_returns.index
            )
        else:
            optimal_weights = pd.Series(result.x, index=expected_returns.index)

        # Apply additional constraints
        optimal_weights = self._apply_risk_constraints(optimal_weights)

        logger.info(f"Portfolio optimization completed. Active positions: {(optimal_weights > 0.01).sum()}")
        return optimal_weights

    def _shrink_covariance(self, cov_matrix: pd.DataFrame) -> pd.DataFrame:
        """Apply covariance shrinkage for stability."""
        # Ledoit-Wolf shrinkage
        target = np.trace(cov_matrix) / len(cov_matrix) * np.eye(len(cov_matrix))
        shrinkage_factor = 0.2

        shrunk_cov = (1 - shrinkage_factor) * cov_matrix + shrinkage_factor * target
        return pd.DataFrame(shrunk_cov, index=cov_matrix.index, columns=cov_matrix.columns)

    def _apply_risk_constraints(self, weights: pd.Series) -> pd.Series:
        """Apply additional risk constraints."""
        # Ensure weights sum to 1
        weights = weights / weights.sum()

        # Apply position size limits
        weights = weights.clip(lower=0, upper=self.config.max_position_size)

        # Re-normalize
        weights = weights / weights.sum()

        return weights


class ProfessionalBacktester:
    """Institutional-grade backtesting engine."""

    def __init__(self, config: TradingConfig, data_manager: DataManager,
                 alpha_model: EliteAlphaModel, risk_manager: AdvancedRiskManager):
        self.config = config
        self.data_manager = data_manager
        self.alpha_model = alpha_model
        self.risk_manager = risk_manager
        self.regime_detector = RegimeDetector(data_manager)

    def run_backtest(self) -> Tuple[pd.Series, Dict[str, pd.DataFrame]]:
        """Run comprehensive backtest with detailed tracking."""
        logger.info("Starting professional backtest...")

        # Initialize data
        returns = self.data_manager.data['returns']
        prices = self.data_manager.data['prices']

        # Generate signals
        signal_generator = AdvancedSignalGenerator(self.data_manager, self.regime_detector)
        signals = signal_generator.generate_all_signals()

        # Prepare ML features
        X, y = self.alpha_model.prepare_features(signals, returns)

        # Train model
        self.alpha_model.train(X, y)

        # Initialize tracking variables
        portfolio_values = []
        weights_history = []
        turnover_history = []
        detailed_stats = {
            'positions': pd.DataFrame(),
            'trades': pd.DataFrame(),
            'risk_metrics': pd.DataFrame(),
            'attribution': pd.DataFrame()
        }

        # Rebalancing dates
        rebalance_dates = self._get_rebalance_dates(returns.index)

        current_weights = pd.Series(0.0, index=returns.columns)
        portfolio_value = 100000  # Starting value

        for i, date in enumerate(rebalance_dates):
            if date not in returns.index:
                continue

            logger.info(f"Rebalancing on {date} ({i+1}/{len(rebalance_dates)})")

            # Get current market data
            current_returns = returns.loc[:date]
            current_prices = prices.loc[:date]

            if len(current_returns) < self.config.lookback_window:
                continue

            # Generate alpha forecast
            feature_data = self._prepare_current_features(signals, date)
            if feature_data is None:
                continue

            alpha_forecast = self.alpha_model.predict(feature_data.unstack().dropna())

            # Estimate covariance matrix
            recent_returns = current_returns.tail(self.config.lookback_window)
            cov_matrix = recent_returns.cov() * 252  # Annualized
            
            # Align alpha forecast and covariance matrix
            common_assets = alpha_forecast.index.intersection(cov_matrix.index)
            alpha_forecast = alpha_forecast.loc[common_assets]
            cov_matrix = cov_matrix.loc[common_assets, common_assets]
            
            # Align current weights
            aligned_current_weights = current_weights.reindex(common_assets).fillna(0.0)


            # Optimize portfolio
            try:
                new_weights_partial = self.risk_manager.optimize_portfolio(
                    alpha_forecast, cov_matrix, aligned_current_weights
                )
                new_weights = pd.Series(0.0, index=returns.columns)
                new_weights.update(new_weights_partial)

            except Exception as e:
                logger.warning(f"Portfolio optimization failed for date {date}: {e}")
                continue

            # Calculate turnover
            turnover = np.sum(np.abs(new_weights - current_weights))

            # Apply transaction costs
            transaction_cost = turnover * self.config.transaction_cost
            
            # Update portfolio value for the period
            if i > 0:
                start_period = rebalance_dates[i-1]
                end_period = date
                period_returns = returns.loc[start_period:end_period]
                
                # Check if there are returns in the period
                if len(period_returns) > 1:
                    # Calculate portfolio returns using previous period's weights
                    portfolio_period_returns = (period_returns * current_weights).sum(axis=1).iloc[1:]
                    
                    # Update portfolio value based on returns
                    for pret in portfolio_period_returns:
                        portfolio_value *= (1 + pret)

            # Apply transaction costs after calculating returns for the period
            portfolio_value *= (1 - transaction_cost)
            
            # Update positions to new weights
            current_weights = new_weights

            # Record keeping
            portfolio_values.append({
                'date': date,
                'portfolio_value': portfolio_value,
                'turnover': turnover,
                'transaction_cost': transaction_cost
            })

            weights_history.append(current_weights.copy())
            turnover_history.append(turnover)


        # Create portfolio return series
        portfolio_df = pd.DataFrame(portfolio_values).set_index('date')
        portfolio_returns = portfolio_df['portfolio_value'].pct_change().dropna()

        # Compile detailed statistics
        valid_rebalance_dates = [d for d in rebalance_dates if d in returns.index and len(returns.loc[:d]) >= self.config.lookback_window]
        detailed_stats['positions'] = pd.DataFrame(weights_history,
                                                  index=valid_rebalance_dates[:len(weights_history)])
        detailed_stats['turnover'] = pd.Series(turnover_history,
                                              index=valid_rebalance_dates[:len(turnover_history)])

        logger.info("Backtest completed successfully")
        return portfolio_returns, detailed_stats

    def _get_rebalance_dates(self, date_index: pd.DatetimeIndex) -> List[pd.Timestamp]:
        """Generate rebalancing dates based on frequency."""
        if self.config.rebalance_frequency == 'M':
            return date_index.to_period('M').to_timestamp('M').unique().tolist()
        elif self.config.rebalance_frequency == 'W':
            return date_index.to_period('W').to_timestamp('W', how='end').unique().tolist()
        elif self.config.rebalance_frequency == 'D':
            return date_index.tolist()
        else: # Quarterly
            return date_index.to_period('Q').to_timestamp('Q').unique().tolist()

    def _prepare_current_features(self, signals: Dict[str, pd.DataFrame],
                                 date: pd.Timestamp) -> Optional[pd.DataFrame]:
        """Prepare features for current date."""
        try:
            # Combine all signal dataframes
            all_signals = pd.concat(signals.values(), axis=1)
            
            if date not in all_signals.index:
                logger.warning(f"No signal data available for {date}")
                return None

            # Get features for the specific date
            current_features = all_signals.loc[[date]]

            # Normalize and create interaction terms as in the training preparation
            current_features = self.alpha_model._create_interaction_terms(current_features)
            current_features = self.alpha_model._normalize_signals(current_features)
            
            return current_features

        except Exception as e:
            logger.warning(f"Feature preparation failed for {date}: {e}")
            return None


class PerformanceAnalyzer:
    """Comprehensive performance analysis and attribution."""

    def __init__(self, config: TradingConfig):
        self.config = config

    def analyze_performance(self, portfolio_returns: pd.Series,
                          benchmark_returns: pd.Series,
                          detailed_stats: Dict[str, pd.DataFrame]) -> Dict[str, float]:
        """Comprehensive performance analysis."""
        logger.info("Analyzing portfolio performance...")

        # Align returns
        aligned_returns = pd.concat([portfolio_returns, benchmark_returns], axis=1).dropna()
        aligned_returns.columns = ['Portfolio', 'Benchmark']
        
        if aligned_returns.empty:
            logger.error("No overlapping returns for analysis")
            return {}

        port_ret = aligned_returns['Portfolio']
        bench_ret = aligned_returns['Benchmark']

        # Basic performance metrics
        metrics = self._calculate_basic_metrics(port_ret, bench_ret)

        # Risk metrics
        risk_metrics = self._calculate_risk_metrics(port_ret, bench_ret)
        metrics.update(risk_metrics)

        # Attribution analysis
        attribution = self._calculate_attribution(port_ret, bench_ret, detailed_stats)
        metrics.update(attribution)

        logger.info("Performance analysis completed")
        return metrics

    def _calculate_basic_metrics(self, portfolio_returns: pd.Series,
                               benchmark_returns: pd.Series) -> Dict[str, float]:
        """Calculate basic performance metrics."""
        metrics = {}

        # Annualized returns
        trading_days = 252
        port_ann_ret = (1 + portfolio_returns).prod() ** (trading_days / len(portfolio_returns)) - 1
        bench_ann_ret = (1 + benchmark_returns).prod() ** (trading_days / len(benchmark_returns)) - 1

        metrics['portfolio_annual_return'] = port_ann_ret
        metrics['benchmark_annual_return'] = bench_ann_ret
        metrics['excess_return'] = port_ann_ret - bench_ann_ret

        # Volatility
        metrics['portfolio_volatility'] = portfolio_returns.std() * np.sqrt(trading_days)
        metrics['benchmark_volatility'] = benchmark_returns.std() * np.sqrt(trading_days)

        # Sharpe ratio
        metrics['portfolio_sharpe'] = (port_ann_ret - self.config.risk_free_rate) / metrics['portfolio_volatility'] if metrics['portfolio_volatility'] > 0 else 0
        metrics['benchmark_sharpe'] = (bench_ann_ret - self.config.risk_free_rate) / metrics['benchmark_volatility'] if metrics['benchmark_volatility'] > 0 else 0


        # Information ratio
        excess_returns = portfolio_returns - benchmark_returns
        tracking_error_std = excess_returns.std()
        metrics['information_ratio'] = (excess_returns.mean() / tracking_error_std) * np.sqrt(trading_days) if tracking_error_std > 0 else 0

        return metrics

    def _calculate_risk_metrics(self, portfolio_returns: pd.Series,
                              benchmark_returns: pd.Series) -> Dict[str, float]:
        """Calculate risk metrics."""
        metrics = {}

        # Beta
        covariance = portfolio_returns.cov(benchmark_returns)
        benchmark_variance = benchmark_returns.var()
        metrics['beta'] = covariance / benchmark_variance if benchmark_variance > 0 else 0

        # Tracking error
        tracking_error = (portfolio_returns - benchmark_returns).std() * np.sqrt(252)
        metrics['tracking_error'] = tracking_error

        # Maximum drawdown
        cumulative_returns = (1 + portfolio_returns).cumprod()
        rolling_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - rolling_max) / rolling_max
        metrics['max_drawdown'] = drawdown.min()

        # VaR (95% confidence)
        metrics['var_95'] = np.percentile(portfolio_returns, 5)

        # Downside deviation
        negative_returns = portfolio_returns[portfolio_returns < 0]
        metrics['downside_deviation'] = negative_returns.std() * np.sqrt(252) if len(negative_returns) > 1 else 0

        return metrics

    def _calculate_attribution(self, portfolio_returns: pd.Series,
                             benchmark_returns: pd.Series,
                             detailed_stats: Dict[str, pd.DataFrame]) -> Dict[str, float]:
        """Calculate performance attribution."""
        attribution = {}

        # Security selection effect (simplified)
        if 'positions' in detailed_stats and not detailed_stats['positions'].empty:
            avg_positions = detailed_stats['positions'].mean()
            attribution['avg_position_count'] = (avg_positions > 0.01).sum()
            attribution['position_concentration'] = (avg_positions ** 2).sum()

        # Turnover analysis
        if 'turnover' in detailed_stats and not detailed_stats['turnover'].empty:
            attribution['avg_turnover'] = detailed_stats['turnover'].mean()
            attribution['turnover_volatility'] = detailed_stats['turnover'].std()

        return attribution

    def generate_report(self, metrics: Dict[str, float]) -> str:
        """Generate formatted performance report."""
        report = "\n" + "="*80 + "\n"
        report += "ELITE QUANTITATIVE TRADING FRAMEWORK - PERFORMANCE REPORT\n"
        report += "="*80 + "\n\n"

        report += "RETURN METRICS:\n"
        report += f"Portfolio Annual Return:    {metrics.get('portfolio_annual_return', 0):.2%}\n"
        report += f"Benchmark Annual Return:    {metrics.get('benchmark_annual_return', 0):.2%}\n"
        report += f"Excess Return:              {metrics.get('excess_return', 0):.2%}\n\n"

        report += "RISK METRICS:\n"
        report += f"Portfolio Volatility:       {metrics.get('portfolio_volatility', 0):.2%}\n"
        report += f"Tracking Error:             {metrics.get('tracking_error', 0):.2%}\n"
        report += f"Maximum Drawdown:           {metrics.get('max_drawdown', 0):.2%}\n"
        report += f"Beta:                       {metrics.get('beta', 0):.3f}\n\n"

        report += "RISK-ADJUSTED METRICS:\n"
        report += f"Portfolio Sharpe Ratio:     {metrics.get('portfolio_sharpe', 0):.3f}\n"
        report += f"Information Ratio:          {metrics.get('information_ratio', 0):.3f}\n"
        report += f"Downside Deviation:         {metrics.get('downside_deviation', 0):.2%}\n\n"

        report += "PORTFOLIO CHARACTERISTICS:\n"
        report += f"Average Position Count:     {metrics.get('avg_position_count', 0):.0f}\n"
        report += f"Average Turnover:           {metrics.get('avg_turnover', 0):.2%}\n"
        report += f"Position Concentration:     {metrics.get('position_concentration', 0):.3f}\n\n"

        report += "="*80 + "\n"

        return report


class AdvancedVisualizer:
    """Professional visualization suite."""

    def __init__(self, config: TradingConfig):
        self.config = config

    def create_performance_dashboard(self, portfolio_returns: pd.Series,
                                   benchmark_returns: pd.Series,
                                   detailed_stats: Dict[str, pd.DataFrame],
                                   metrics: Dict[str, float],
                                   output_path: Path) -> None:
        """Create comprehensive performance dashboard."""
        logger.info("Creating performance dashboard...")

        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Elite Quantitative Trading Framework - Performance Dashboard',
                    fontsize=16, fontweight='bold')

        # Align data for plotting
        aligned_returns = pd.concat([portfolio_returns, benchmark_returns], axis=1).dropna()
        aligned_returns.columns = ['Portfolio', 'Benchmark']
        port_ret = aligned_returns['Portfolio']
        bench_ret = aligned_returns['Benchmark']


        # 1. Cumulative returns
        self._plot_cumulative_returns(axes[0, 0], port_ret, bench_ret)

        # 2. Rolling Sharpe ratio
        self._plot_rolling_sharpe(axes[0, 1], port_ret, bench_ret)

        # 3. Drawdown analysis
        self._plot_drawdown(axes[0, 2], port_ret)

        # 4. Return distribution
        self._plot_return_distribution(axes[1, 0], port_ret, bench_ret)

        # 5. Portfolio composition
        self._plot_portfolio_composition(axes[1, 1], detailed_stats)

        # 6. Risk metrics summary
        self._plot_risk_summary(axes[1, 2], metrics)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close(fig) # Close the figure to free up memory

        logger.info(f"Performance dashboard saved to {output_path}")

    def _plot_cumulative_returns(self, ax, portfolio_returns: pd.Series,
                               benchmark_returns: pd.Series) -> None:
        """Plot cumulative returns comparison."""
        cum_port = (1 + portfolio_returns).cumprod()
        cum_bench = (1 + benchmark_returns).cumprod()

        ax.plot(cum_port.index, cum_port.values, label='Portfolio', linewidth=2, color='blue')
        ax.plot(cum_bench.index, cum_bench.values, label='Benchmark', linewidth=2, color='red')
        ax.set_title('Cumulative Returns', fontweight='bold')
        ax.set_ylabel('Cumulative Return')
        ax.legend()
        ax.grid(True, alpha=0.3)

    def _plot_rolling_sharpe(self, ax, portfolio_returns: pd.Series,
                           benchmark_returns: pd.Series) -> None:
        """Plot rolling Sharpe ratio."""
        rolling_window = 60
        
        port_rolling_mean = portfolio_returns.rolling(rolling_window).mean()
        port_rolling_std = portfolio_returns.rolling(rolling_window).std()
        port_rolling_sharpe = (port_rolling_mean / port_rolling_std) * np.sqrt(252)
        
        bench_rolling_mean = benchmark_returns.rolling(rolling_window).mean()
        bench_rolling_std = benchmark_returns.rolling(rolling_window).std()
        bench_rolling_sharpe = (bench_rolling_mean / bench_rolling_std) * np.sqrt(252)

        ax.plot(port_rolling_sharpe.index, port_rolling_sharpe.values,
               label='Portfolio', linewidth=2, color='blue')
        ax.plot(bench_rolling_sharpe.index, bench_rolling_sharpe.values,
               label='Benchmark', linewidth=2, color='red')
        ax.set_title('Rolling Sharpe Ratio (60-day)', fontweight='bold')
        ax.set_ylabel('Sharpe Ratio')
        ax.legend()
        ax.grid(True, alpha=0.3)

    def _plot_drawdown(self, ax, portfolio_returns: pd.Series) -> None:
        """Plot drawdown analysis."""
        cumulative = (1 + portfolio_returns).cumprod()
        rolling_max = cumulative.expanding().max()
        drawdown = (cumulative - rolling_max) / rolling_max

        ax.fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, color='red')
        ax.plot(drawdown.index, drawdown.values, color='red', linewidth=1)
        ax.set_title('Portfolio Drawdown', fontweight='bold')
        ax.set_ylabel('Drawdown')
        ax.grid(True, alpha=0.3)

    def _plot_return_distribution(self, ax, portfolio_returns: pd.Series,
                                benchmark_returns: pd.Series) -> None:
        """Plot return distribution comparison."""
        sns.histplot(portfolio_returns, bins=50, alpha=0.7, label='Portfolio',
                     color='blue', density=True, ax=ax, kde=True)
        sns.histplot(benchmark_returns, bins=50, alpha=0.7, label='Benchmark',
                     color='red', density=True, ax=ax, kde=True)
        ax.set_title('Return Distribution', fontweight='bold')
        ax.set_xlabel('Daily Returns')
        ax.set_ylabel('Density')
        ax.legend()
        ax.grid(True, alpha=0.3)

    def _plot_portfolio_composition(self, ax, detailed_stats: Dict[str, pd.DataFrame]) -> None:
        """Plot portfolio composition over time."""
        if 'positions' in detailed_stats and not detailed_stats['positions'].empty:
            positions = detailed_stats['positions'].copy()
            positions = positions.loc[:, (positions != 0).any(axis=0)] # Drop empty columns

            # Plot top 5 average positions over time
            avg_positions = positions.mean().sort_values(ascending=False)
            top_assets = avg_positions.head(5).index

            positions[top_assets].plot.area(ax=ax, stacked=True, linewidth=0)
            
            ax.set_title('Top 5 Portfolio Positions', fontweight='bold')
            ax.set_ylabel('Weight')
            ax.legend(loc='upper left')
            ax.grid(True, alpha=0.3)
        else:
            ax.text(0.5, 0.5, 'No position data available',
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Portfolio Composition', fontweight='bold')

    def _plot_risk_summary(self, ax, metrics: Dict[str, float]) -> None:
        """Plot risk metrics summary."""
        risk_metrics_data = {
            'Volatility': metrics.get('portfolio_volatility', 0),
            'Max Drawdown': abs(metrics.get('max_drawdown', 0)),
            'Tracking Error': metrics.get('tracking_error', 0),
            'Downside Dev': metrics.get('downside_deviation', 0)
        }

        names = list(risk_metrics_data.keys())
        values = list(risk_metrics_data.values())

        bars = ax.bar(names, values, color=['#1f77b4', '#d62728', '#2ca02c', '#ff7f0e'])
        ax.set_title('Risk Metrics Summary', fontweight='bold')
        ax.set_ylabel('Value (as %)')
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))


        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.2%}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom')

        ax.grid(True, axis='y', alpha=0.3)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

class TelegramNotifier:
    """Handles sending notifications to a Telegram bot."""

    def __init__(self, token: str, chat_id: str):
        if not telegram:
            raise ImportError("The 'python-telegram-bot' library is required for Telegram notifications.")
        self.bot = telegram.Bot(token=token)
        self.chat_id = chat_id
        logger.info(f"TelegramNotifier initialized for chat_id: {chat_id}")

    async def send_message(self, text: str):
        """Sends a text message to the configured chat."""
        try:
            logger.info("Sending performance report to Telegram...")
            # Telegram has a message size limit of 4096 characters
            max_len = 4096
            for i in range(0, len(text), max_len):
                chunk = text[i:i + max_len]
                await self.bot.send_message(chat_id=self.chat_id, text=f'<pre>{chunk}</pre>', parse_mode='HTML')
            logger.info("Telegram message sent successfully.")
        except Exception as e:
            logger.error(f"Failed to send Telegram message: {e}")

    async def send_photo(self, photo_path: Path, caption: str):
        """Sends a photo to the configured chat."""
        if not photo_path.exists():
            logger.error(f"Photo path for Telegram does not exist: {photo_path}")
            return
        try:
            logger.info("Sending performance dashboard to Telegram...")
            await self.bot.send_photo(chat_id=self.chat_id, photo=open(photo_path, 'rb'), caption=caption)
            logger.info("Telegram photo sent successfully.")
        except Exception as e:
            logger.error(f"Failed to send Telegram photo: {e}")


async def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description='Elite Quantitative Trading Framework')
    parser.add_argument('--config', type=str, help='Configuration file path')
    parser.add_argument('--symbols', nargs='+', help='Trading symbols')
    parser.add_argument('--start-date', type=str, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='End date (YYYY-MM-DD)')
    parser.add_argument('--output-dir', type=str, default='output', help='Output directory')
    parser.add_argument('--telegram-token', type=str, help='Telegram Bot Token')
    parser.add_argument('--telegram-chat-id', type=str, help='Telegram Chat ID')


    args = parser.parse_args()

    # Initialize configuration
    config = TradingConfig()

    # Override with command line arguments
    if args.symbols:
        config.universe = args.symbols
    if args.start_date:
        config.start_date = args.start_date
    if args.end_date:
        config.end_date = args.end_date
    if args.telegram_token:
        config.telegram_bot_token = args.telegram_token
    if args.telegram_chat_id:
        config.telegram_chat_id = args.telegram_chat_id


    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    try:
        logger.info("Starting Elite Quantitative Trading Framework v2.0")
        logger.info(f"Configuration: {len(config.universe)} assets, {config.start_date} to {config.end_date}")

        # Initialize components
        data_manager = DataManager(config)
        alpha_model = EliteAlphaModel(config)
        risk_manager = AdvancedRiskManager(config)

        # Fetch and process data
        data = data_manager.fetch_data()

        # Initialize backtester
        backtester = ProfessionalBacktester(config, data_manager, alpha_model, risk_manager)

        # Run backtest
        portfolio_returns, detailed_stats = backtester.run_backtest()

        # Get benchmark returns
        benchmark_returns = data['returns'][config.benchmark]

        # Analyze performance
        analyzer = PerformanceAnalyzer(config)
        metrics = analyzer.analyze_performance(portfolio_returns, benchmark_returns, detailed_stats)

        # Generate report
        report = analyzer.generate_report(metrics)
        print(report)

        # Save report
        report_path = output_dir / 'performance_report.txt'
        with open(report_path, 'w') as f:
            f.write(report)
        logger.info(f"Performance report saved to {report_path}")

        # Create visualizations
        dashboard_path = output_dir / 'performance_dashboard.png'
        visualizer = AdvancedVisualizer(config)
        visualizer.create_performance_dashboard(portfolio_returns, benchmark_returns,
                                              detailed_stats, metrics, dashboard_path)

        # Save results
        portfolio_returns.to_csv(output_dir / 'portfolio_returns.csv')
        benchmark_returns.to_csv(output_dir / 'benchmark_returns.csv')

        if 'positions' in detailed_stats and not detailed_stats['positions'].empty:
            detailed_stats['positions'].to_csv(output_dir / 'positions.csv')

        # Send Telegram Notification if configured
        if config.telegram_bot_token and config.telegram_chat_id:
            notifier = TelegramNotifier(token=config.telegram_bot_token, chat_id=config.telegram_chat_id)
            await notifier.send_message(report)
            await notifier.send_photo(dashboard_path, caption="Trading Strategy Performance Dashboard")


        logger.info("Elite Quantitative Trading Framework completed successfully")
        logger.info(f"Results saved to: {output_dir}")

    except Exception as e:
        logger.error(f"Framework execution failed: {e}", exc_info=True)
        # Optionally send an error message via Telegram
        if config.telegram_bot_token and config.telegram_chat_id:
            try:
                notifier = TelegramNotifier(token=config.telegram_bot_token, chat_id=config.telegram_chat_id)
                await notifier.send_message(f"🚨 Framework execution failed!\n\nError: {e}")
            except Exception as telegram_err:
                logger.error(f"Could not send error notification via Telegram: {telegram_err}")
        raise


if __name__ == "__main__":
    import asyncio
    # In Python 3.8+ on Windows, the default event loop policy is changed.
    # The following line is needed to run `telegram` async functions in a script.
    if sys.platform == 'win32' and sys.version_info >= (3, 8):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
    asyncio.run(main())
